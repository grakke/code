{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本特征提取"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 简单的情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'I': True, 'like': True, 'movie': True, 'much': True, '!': True}, 1], [{'That': True, 'good': True, 'movie': True, '.': True}, 1], [{'This': True, 'great': True, 'one': True, '.': True}, 1], [{'That': True, 'really': True, 'bad': True, 'movie': True, '.': True}, 0], [{'This': True, 'terrible': True, 'movie': True, '.': True}, 0]]\n"
     ]
    }
   ],
   "source": [
    "# 简单的例子\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "text1 = 'I like the movie so much!'\n",
    "text2 = 'That is a good movie.'\n",
    "text3 = 'This is a great one.'\n",
    "text4 = 'That is a really bad movie.'\n",
    "text5 = 'This is a terrible movie.'\n",
    "\n",
    "def proc_text(text):\n",
    "    \"\"\"\n",
    "        预处处理文本\n",
    "    \"\"\"\n",
    "    # 分词\n",
    "    raw_words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # 词形归一化\n",
    "    wordnet_lematizer = WordNetLemmatizer()    \n",
    "    words = [wordnet_lematizer.lemmatize(raw_word) for raw_word in raw_words]\n",
    "    \n",
    "    # 去除停用词\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    # True 表示该词在文本中，为了使用nltk中的分类器\n",
    "    return {word: True for word in filtered_words}\n",
    "\n",
    "# 构造训练样本\n",
    "train_data = [[proc_text(text1), 1],\n",
    "              [proc_text(text2), 1],\n",
    "              [proc_text(text3), 1],\n",
    "              [proc_text(text4), 0],\n",
    "              [proc_text(text5), 0]]\n",
    "\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "nb_model = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# 测试模型\n",
    "text6 = 'That is a not bad one.'\n",
    "print(nb_model.classify(proc_text(text6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import re\n",
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词表\n",
    "stop_words_path = './stop_words/'\n",
    "\n",
    "stopwords1 = [line.rstrip() for line in open(os.path.join(stop_words_path, '中文停用词库.txt'), 'r',\n",
    "                                             encoding='utf-8')]\n",
    "stopwords2 = [line.rstrip() for line in open(os.path.join(stop_words_path, '哈工大停用词表.txt'), 'r',\n",
    "                                             encoding='utf-8')]\n",
    "stopwords3 = [line.rstrip() for line in\n",
    "              open(os.path.join(stop_words_path, '四川大学机器智能实验室停用词库.txt'), 'r', encoding='utf-8')]\n",
    "stopwords = stopwords1 + stopwords2 + stopwords3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_text(raw_line):\n",
    "    \"\"\"\n",
    "        处理文本数据\n",
    "        返回分词结果\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 使用正则表达式去除非中文字符\n",
    "    filter_pattern = re.compile('[^\\u4E00-\\u9FD5]+')\n",
    "    chinese_only = filter_pattern.sub('', raw_line)\n",
    "\n",
    "    # 2. 结巴分词+词性标注\n",
    "    word_list = pseg.cut(chinese_only)\n",
    "\n",
    "    # 3. 去除停用词，保留有意义的词性\n",
    "    # 动词，形容词，副词\n",
    "    used_flags = ['v', 'a', 'ad', 'n']\n",
    "    meaninful_words = []\n",
    "    for word, flag in word_list:\n",
    "        if (word not in stopwords) and (flag in used_flags):\n",
    "            meaninful_words.append(word)\n",
    "    return ' '.join(meaninful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 处理训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_text1 = '5月9日，新华社发文称，五一前后住建部约谈了成都、太原等12个城市，主要集中在东北、中西部地区以及政策利好的海南。'\n",
    "ch_text2 = '响应国家号召，被约谈城市纷纷出台举措应对楼市过热问题。'\n",
    "ch_text3 = '成都由此将限购对象从自然人升级为家庭，西安对碧桂园等42家房企进行约谈，昆明、贵阳、长春则升级了限售政策。'\n",
    "ch_text4 = '5月份楼市调控的高潮出现在19日，住建部再次重申坚持房地产调控目标不动摇、力度不放松，并从住房发展规划、住房和土地供应、资金管控、市场监管、落实主体责任等方面做出了明确要求。'\n",
    "ch_text5 = '调控的目的只有一个，那就是稳定楼市。'\n",
    "\n",
    "ch_texts = [ch_text1, ch_text2, ch_text3, ch_text4, ch_text5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Robin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.756 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "corpus = [proc_text(ch_text) for ch_text in ch_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['发文 称 住 建部 谈 集中 地区 政策 利好',\n",
       " '响应 国家 号召 谈 出台 举措 应对 楼市 过热 问题',\n",
       " '限购 对象 家庭 等家 进行 谈 限售 政策',\n",
       " '月份 楼市 出现 住 建部 重申 坚持 目标 动摇 力度 放松 住房 规划 住房 土地 资金 市场监管 落实 主体 责任 方面 做出',\n",
       " '目的 稳定 楼市']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 建立BoW模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=10, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=10)\n",
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = count_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'建部': 1,\n",
       " '政策': 2,\n",
       " '楼市': 5,\n",
       " '等家': 9,\n",
       " '月份': 4,\n",
       " '目标': 6,\n",
       " '住房': 0,\n",
       " '方面': 3,\n",
       " '目的': 7,\n",
       " '稳定': 8}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看词典\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [2, 1, 0, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 处理新文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'面对 市场 双面 性 关心 做出 购房 选择 楼市 看待 股市 声音 声音 认为 楼市 要涨 上车 声音 认为 楼市 求稳 学会 飞 面对 声音 喜欢 论出 是非 胜负 坦然 愿意 踏实 努力 回头 需 上车 炒 房 终会 报告 明确 加快 建立 主体 保障 租 购并 举 住房 制度'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = '''面对调控与市场的双面性，人们最关心的是如何做出购房选择。预判楼市如同看待股市，总有两种不同声音。\n",
    "\n",
    "　　一种声音认为，楼市要涨，再不上车就“晚”了；一种声音认为，楼市求稳，要学会慢慢的“飞”。\n",
    "\n",
    "　　面对两种声音，较真儿的人总喜欢论出个是非胜负，坦然的人更愿意踏实努力尽力而为。\n",
    "\n",
    "　　回头看，刚需的人总要上车，被炒的房终会入市。\n",
    "\n",
    "　　十九大报告已经明确“要加快建立多主体供应、多渠道保障，租购并举的住房制度”。'''\n",
    "new_pro_text = proc_text(new_text)\n",
    "new_pro_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 3, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.transform([new_pro_text]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 文本分类及TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 NLTK中的TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That的TF-IDF值为：0.02181644599700369\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import TextCollection\n",
    "\n",
    "text1 = 'I like the movie so much '\n",
    "text2 = 'That is a good movie '\n",
    "text3 = 'This is a great one '\n",
    "text4 = 'That is a really bad movie '\n",
    "text5 = 'This is a terrible movie'\n",
    "\n",
    "# 构建TextCollection对象\n",
    "tc = TextCollection([text1, text2, text3, \n",
    "                        text4, text5])\n",
    "new_text = 'That one is a good movie. This is so good!'\n",
    "word = 'That'\n",
    "tf_idf_val = tc.tf_idf(word, new_text)\n",
    "print('{}的TF-IDF值为：{}'.format(word, tf_idf_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 sklearn中的TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "feat = vectorizer.fit_transform([text1, text2, text3, text4, text5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.48127008,\n",
       "        0.27113917, 0.48127008, 0.        , 0.        , 0.48127008,\n",
       "        0.        , 0.        , 0.48127008, 0.        ],\n",
       "       [0.        , 0.6614376 , 0.        , 0.3726424 , 0.        ,\n",
       "        0.3726424 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.53364369, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.58042343, 0.32700044, 0.        ,\n",
       "        0.        , 0.        , 0.58042343, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.46828197],\n",
       "       [0.55167715, 0.        , 0.        , 0.31080528, 0.        ,\n",
       "        0.31080528, 0.        , 0.        , 0.55167715, 0.        ,\n",
       "        0.        , 0.44508965, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.3726424 , 0.        ,\n",
       "        0.3726424 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.6614376 , 0.        , 0.        , 0.53364369]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad',\n",
       " 'good',\n",
       " 'great',\n",
       " 'is',\n",
       " 'like',\n",
       " 'movie',\n",
       " 'much',\n",
       " 'one',\n",
       " 'really',\n",
       " 'so',\n",
       " 'terrible',\n",
       " 'that',\n",
       " 'the',\n",
       " 'this']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 14)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_array = feat.toarray()\n",
    "feat_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.48127008,\n",
       "        0.27113917, 0.48127008, 0.        , 0.        , 0.48127008,\n",
       "        0.        , 0.        , 0.48127008, 0.        ],\n",
       "       [0.        , 0.6614376 , 0.        , 0.3726424 , 0.        ,\n",
       "        0.3726424 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.53364369, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_array[0:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.67082255, 0.        , 0.37792972, 0.        ,\n",
       "        0.18896486, 0.        , 0.33541128, 0.        , 0.33541128,\n",
       "        0.        , 0.27060771, 0.        , 0.27060771]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([new_text]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 中文TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "ch_text1 = '5月9日，新华社发文称，五一前后住建部约谈了成都、太原等12个城市，主要集中在东北、中西部地区以及政策利好的海南。'\n",
    "ch_text2 = '响应国家号召，被约谈城市纷纷出台举措应对楼市过热问题。'\n",
    "ch_text3 = '成都由此将限购对象从自然人升级为家庭，西安对碧桂园等42家房企进行约谈，昆明、贵阳、长春则升级了限售政策。'\n",
    "ch_text4 = '5月份楼市调控的高潮出现在19日，住建部再次重申坚持房地产调控目标不动摇、力度不放松，并从住房发展规划、住房和土地供应、资金管控、市场监管、落实主体责任等方面做出了明确要求。'\n",
    "ch_text5 = '调控的目的只有一个，那就是稳定楼市。'\n",
    "\n",
    "ch_texts = [ch_text1, ch_text2, ch_text3, ch_text4, ch_text5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载停用词表\n",
    "stop_words_path = './stop_words/'\n",
    "\n",
    "stopwords1 = [line.rstrip() for line in open(os.path.join(stop_words_path, '中文停用词库.txt'), 'r',\n",
    "                                             encoding='utf-8')]\n",
    "stopwords2 = [line.rstrip() for line in open(os.path.join(stop_words_path, '哈工大停用词表.txt'), 'r',\n",
    "                                             encoding='utf-8')]\n",
    "stopwords3 = [line.rstrip() for line in\n",
    "              open(os.path.join(stop_words_path, '四川大学机器智能实验室停用词库.txt'), 'r', encoding='utf-8')]\n",
    "stopwords = stopwords1 + stopwords2 + stopwords3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2489"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proc_text(raw_line):\n",
    "    \"\"\"\n",
    "        处理文本数据\n",
    "        返回分词结果\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 使用正则表达式去除非中文字符\n",
    "    filter_pattern = re.compile('[^\\u4E00-\\u9FD5]+')\n",
    "    chinese_only = filter_pattern.sub('', raw_line)\n",
    "\n",
    "    # 2. 结巴分词+词性标注\n",
    "    word_list = pseg.cut(chinese_only)\n",
    "\n",
    "    # 3. 去除停用词，保留有意义的词性\n",
    "    # 动词，形容词，副词\n",
    "    used_flags = ['v', 'a', 'ad']\n",
    "    meaninful_words = []\n",
    "    for word, flag in word_list:\n",
    "#         if (word not in stopwords) and (flag in used_flags):\n",
    "        if word not in stopwords:\n",
    "            meaninful_words.append(word)\n",
    "    return ' '.join(meaninful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [proc_text(ch_text) for ch_text in ch_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['月 日 新华社 发文 称 五一 住 建部 约 谈 成都 太原 城市 主要 集中 东北 中西部 地区 政策 利好 海南',\n",
       " '响应 国家 号召 约 谈 城市 纷纷 出台 举措 应对 楼市 过热 问题',\n",
       " '成都 限购 对象 自然人 升级 家庭 西安 碧桂园 等家 房企 进行 约 谈 昆明 贵阳 长春 升级 限售 政策',\n",
       " '月份 楼市 调控 高潮 出现 日 住 建部 重申 坚持 房地产 调控 目标 动摇 力度 放松 住房 发展 规划 住房 土地 供应 资金 管控 市场监管 落实 主体 责任 方面 做出 明确要求',\n",
       " '调控 目的 稳定 楼市']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "ch_vectorizer = TfidfVectorizer()\n",
    "ch_feats = ch_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['东北',\n",
       " '中西部',\n",
       " '主体',\n",
       " '主要',\n",
       " '举措',\n",
       " '五一',\n",
       " '住房',\n",
       " '供应',\n",
       " '做出',\n",
       " '出台',\n",
       " '出现',\n",
       " '利好',\n",
       " '力度',\n",
       " '动摇',\n",
       " '升级',\n",
       " '发展',\n",
       " '发文',\n",
       " '号召',\n",
       " '响应',\n",
       " '国家',\n",
       " '土地',\n",
       " '地区',\n",
       " '坚持',\n",
       " '城市',\n",
       " '太原',\n",
       " '家庭',\n",
       " '对象',\n",
       " '市场监管',\n",
       " '应对',\n",
       " '建部',\n",
       " '成都',\n",
       " '房企',\n",
       " '房地产',\n",
       " '放松',\n",
       " '政策',\n",
       " '新华社',\n",
       " '方面',\n",
       " '昆明',\n",
       " '明确要求',\n",
       " '月份',\n",
       " '楼市',\n",
       " '海南',\n",
       " '目标',\n",
       " '目的',\n",
       " '碧桂园',\n",
       " '稳定',\n",
       " '等家',\n",
       " '管控',\n",
       " '纷纷',\n",
       " '自然人',\n",
       " '落实',\n",
       " '西安',\n",
       " '规划',\n",
       " '调控',\n",
       " '责任',\n",
       " '贵阳',\n",
       " '资金',\n",
       " '过热',\n",
       " '进行',\n",
       " '重申',\n",
       " '长春',\n",
       " '问题',\n",
       " '限售',\n",
       " '限购',\n",
       " '集中',\n",
       " '高潮']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27112655, 0.27112655, 0.        , 0.27112655, 0.        ,\n",
       "       0.27112655, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.27112655, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.27112655, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.27112655, 0.        , 0.21874319, 0.27112655,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.21874319,\n",
       "       0.21874319, 0.        , 0.        , 0.        , 0.21874319,\n",
       "       0.27112655, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.27112655, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.27112655,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_feats.toarray()[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
