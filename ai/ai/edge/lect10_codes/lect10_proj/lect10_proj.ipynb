{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # 批大小\n",
    "epochs = 100     # 迭代次数\n",
    "latent_dim = 256 # 隐含层节点个数\n",
    "n_samples = 10000 # 样本数量\n",
    "\n",
    "# 数据路径\n",
    "data_path = './cmn-eng/cmn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_chars = set()\n",
    "target_chars = set()\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "for line in lines[: min(n_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    # 目标句子以'\\t'作为开始字符，'\\n'结束\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    # 将input的字符放入input字符集\n",
    "    for char in input_text:\n",
    "        if char not in input_chars:\n",
    "            input_chars.add(char)\n",
    "    \n",
    "    # 将target的字符放入target字符集\n",
    "    for char in target_text:\n",
    "        if char not in target_chars:\n",
    "            target_chars.add(char)\n",
    "\n",
    "# 将集合中的字符进行排序\n",
    "input_chars = sorted(list(input_chars))\n",
    "target_chars = sorted(list(target_chars))\n",
    "\n",
    "n_encoder_tokens = len(input_chars)  # input字符集大小\n",
    "n_decoder_tokens = len(target_chars) # target字符集大小 \n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])  # input字符串的最大长度\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts]) # target字符串的最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数量： 10000\n",
      "不同的输入字符个数： 73\n",
      "不同的输出字符个数： 2623\n",
      "输入序列最大长度： 30\n",
      "输出序列最大长度： 22\n"
     ]
    }
   ],
   "source": [
    "print('样本数量：', len(input_texts))\n",
    "print('不同的输入字符个数：', n_encoder_tokens)\n",
    "print('不同的输出字符个数：', n_decoder_tokens)\n",
    "print('输入序列最大长度：', max_encoder_seq_length)\n",
    "print('输出序列最大长度：', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造字符集字典，每个字符对应一个编号，如 'H'->0, 'e'->1, ...\n",
    "\n",
    "# 构造input字符集字典\n",
    "input_token_dict = dict(\n",
    "    [(char, i) for i, char in enumerate(input_chars)])\n",
    "# 构造target字符集字典\n",
    "target_token_dict = dict(\n",
    "    [(char, i) for i, char in enumerate(target_chars)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对序列每个时刻字符的对应位置设为1\n",
    "\n",
    "# 数据初始化，形状：(样本数量 x 序列最大长度 x 字符个数)\n",
    "# 初始化编码的输入数据\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, n_encoder_tokens), \n",
    "    dtype='float32')\n",
    "\n",
    "# 初始化解码的输入数据\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, n_decoder_tokens), \n",
    "    dtype='float32')\n",
    "\n",
    "# 初始化解码的输出数据\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, n_decoder_tokens), \n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    # 将编码输入数据t时刻的对应字符的位置设为1\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_dict[char]] = 1\n",
    "        \n",
    "    for t, char in enumerate(target_text):\n",
    "        # 将解码输入数据t时刻的对应字符的位置设为1\n",
    "        decoder_input_data[i, t, target_token_dict[char]] = 1\n",
    "        if t > 0:\n",
    "            # 将解码输出数据t-1时刻的对应字符的位置设为1\n",
    "            decoder_target_data[i, t - 1, target_token_dict[char]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义编码模型\n",
    "encoder_inputs = Input(shape=(None, n_encoder_tokens), name='enc_input_layer')\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, name='enc_lstm_layer')\n",
    "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义解码模型\n",
    "decoder_inputs = Input(shape=(None, n_decoder_tokens), name='dec_input_layer')\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='dec_lstm_layer')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(n_decoder_tokens, activation='softmax', name='dec_dense_layer')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_input_layer (InputLayer)    (None, None, 73)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_input_layer (InputLayer)    (None, None, 2623)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_lstm_layer (LSTM)           [(None, 256), (None, 337920      enc_input_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dec_lstm_layer (LSTM)           [(None, None, 256),  2949120     dec_input_layer[0][0]            \n",
      "                                                                 enc_lstm_layer[0][1]             \n",
      "                                                                 enc_lstm_layer[0][2]             \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_layer (Dense)         (None, None, 2623)   674111      dec_lstm_layer[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 3,961,151\n",
      "Trainable params: 3,961,151\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 109s 14ms/step - loss: 2.0339 - val_loss: 2.4937\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 1.8995 - val_loss: 2.4177\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.7902 - val_loss: 2.3173\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.7137 - val_loss: 2.2279\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.6423 - val_loss: 2.1755\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.5857 - val_loss: 2.1170\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.5287 - val_loss: 2.0744\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.4812 - val_loss: 2.0511\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 105s 13ms/step - loss: 1.4276 - val_loss: 1.9862\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.3729 - val_loss: 1.9523\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.3189 - val_loss: 1.9117\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.2759 - val_loss: 1.8778\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 1.2307 - val_loss: 1.8657\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.1942 - val_loss: 1.8529\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.1558 - val_loss: 1.8386\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.1219 - val_loss: 1.8129\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.0924 - val_loss: 1.7940\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.0612 - val_loss: 1.8100\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 1.0322 - val_loss: 1.7946\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 1.0027 - val_loss: 1.7798\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.9726 - val_loss: 1.7682\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.9445 - val_loss: 1.7766\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.9187 - val_loss: 1.7800\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 106s 13ms/step - loss: 0.8918 - val_loss: 1.7644\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 106s 13ms/step - loss: 0.8663 - val_loss: 1.7801\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.8423 - val_loss: 1.7678\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.8171 - val_loss: 1.7751\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.7944 - val_loss: 1.7733\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.7714 - val_loss: 1.7786\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.7491 - val_loss: 1.7814\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.7267 - val_loss: 1.7766\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.7060 - val_loss: 1.7925\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.6848 - val_loss: 1.8069\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.6651 - val_loss: 1.7990\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.6451 - val_loss: 1.8082\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.6261 - val_loss: 1.8115\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.6077 - val_loss: 1.8155\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.5889 - val_loss: 1.8270\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 104s 13ms/step - loss: 0.5719 - val_loss: 1.8421\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.5535 - val_loss: 1.8430\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.5372 - val_loss: 1.8441\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.5207 - val_loss: 1.8556\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.5042 - val_loss: 1.8678\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.4894 - val_loss: 1.8674\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 104s 13ms/step - loss: 0.4735 - val_loss: 1.8727\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.4584 - val_loss: 1.8799\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.4443 - val_loss: 1.8962\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.4295 - val_loss: 1.8978\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 106s 13ms/step - loss: 0.4156 - val_loss: 1.9135\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 105s 13ms/step - loss: 0.4015 - val_loss: 1.9140\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.3878 - val_loss: 1.9359\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.3754 - val_loss: 1.9382\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.3631 - val_loss: 1.9596\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.3501 - val_loss: 1.9557\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.3386 - val_loss: 1.9611\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.3273 - val_loss: 1.9762\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.3160 - val_loss: 1.9824\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.3045 - val_loss: 1.9977\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.2938 - val_loss: 1.9982\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.2831 - val_loss: 2.0079\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.2732 - val_loss: 2.0136\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.2625 - val_loss: 2.0278\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.2537 - val_loss: 2.0409\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.2444 - val_loss: 2.0538\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.2350 - val_loss: 2.0540\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.2265 - val_loss: 2.0699\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.2170 - val_loss: 2.0758\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.2088 - val_loss: 2.0900\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.2014 - val_loss: 2.1078\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.1934 - val_loss: 2.1200\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.1852 - val_loss: 2.1238\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.1779 - val_loss: 2.1249\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 105s 13ms/step - loss: 0.1705 - val_loss: 2.1225\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.1638 - val_loss: 2.1436\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.1574 - val_loss: 2.1587\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.1508 - val_loss: 2.1691\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.1443 - val_loss: 2.1783\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.1383 - val_loss: 2.1855\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.1328 - val_loss: 2.2037\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.1267 - val_loss: 2.2079\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.1210 - val_loss: 2.2152\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.1156 - val_loss: 2.2277\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.1106 - val_loss: 2.2324\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.1059 - val_loss: 2.2521\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 104s 13ms/step - loss: 0.1016 - val_loss: 2.2560\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.0960 - val_loss: 2.2600\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.0924 - val_loss: 2.2566\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.0880 - val_loss: 2.2702\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.0842 - val_loss: 2.2883\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.0806 - val_loss: 2.2921\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.0772 - val_loss: 2.2966\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.0741 - val_loss: 2.3139\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 108s 13ms/step - loss: 0.0703 - val_loss: 2.3173\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 111s 14ms/step - loss: 0.0677 - val_loss: 2.3159\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 111s 14ms/step - loss: 0.0649 - val_loss: 2.3211\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 104s 13ms/step - loss: 0.0610 - val_loss: 2.3390\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.0588 - val_loss: 2.3431\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.0565 - val_loss: 2.3536\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.0534 - val_loss: 2.3565\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 102s 13ms/step - loss: 0.0521 - val_loss: 2.3601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\keras\\engine\\topology.py:2368: UserWarning: Layer dec_lstm_layer was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'enc_lstm_layer/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'enc_lstm_layer/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "         batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "# 保存模型\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "latent_dim = 256 # 隐含层节点个数\n",
    "\n",
    "model = load_model('s2s.h5')\n",
    "\n",
    "# 加载编码模型\n",
    "encoder_inputs = model.inputs[0]\n",
    "encoder_lstm = model.get_layer('enc_lstm_layer')\n",
    "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 加载解码模型\n",
    "decoder_inputs = Input(shape=(None, n_decoder_tokens))\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.get_layer('dec_lstm_layer')\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_dense = model.get_layer('dec_dense_layer')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建反向字符集字典，如：0->'H', 1->'e', ...\n",
    "reverse_target_char_dict = dict((i, char) for char, i in target_token_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    \"\"\"\n",
    "        根据输入字符串，输出翻译的字符串\n",
    "    \"\"\"\n",
    "    # 将input编码为 state_h 和 state_c\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 初始化target序列\n",
    "    target_seq = np.zeros((1, 1, n_decoder_tokens))\n",
    "    # 将第一个字符设为'\\t'\n",
    "    target_seq[0, 0, target_token_dict['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        # 根据target_seq的输入和states_value进行解码\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # 找到概率最大的target字符的编号\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # 将编号转换为字符\n",
    "        sampled_char = reverse_target_char_dict[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 更新target字符串\n",
    "        target_seq = np.zeros((1, 1, n_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 更新状态\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "英文输入: Hi.\n",
      "中文翻译: 你好。\n",
      "\n",
      "-\n",
      "英文输入: Hi.\n",
      "中文翻译: 你好。\n",
      "\n",
      "-\n",
      "英文输入: Run.\n",
      "中文翻译: 你用跑的。\n",
      "\n",
      "-\n",
      "英文输入: Wait!\n",
      "中文翻译: 等等！\n",
      "\n",
      "-\n",
      "英文输入: Hello!\n",
      "中文翻译: 你好。\n",
      "\n",
      "-\n",
      "英文输入: I try.\n",
      "中文翻译: 让我来。\n",
      "\n",
      "-\n",
      "英文输入: I won!\n",
      "中文翻译: 我赢了。\n",
      "\n",
      "-\n",
      "英文输入: Oh no!\n",
      "中文翻译: 不会吧。\n",
      "\n",
      "-\n",
      "英文输入: Cheers!\n",
      "中文翻译: 乾杯!\n",
      "\n",
      "-\n",
      "英文输入: He ran.\n",
      "中文翻译: 他跑了。\n",
      "\n",
      "-\n",
      "英文输入: Hop in.\n",
      "中文翻译: 跳进来。\n",
      "\n",
      "-\n",
      "英文输入: I lost.\n",
      "中文翻译: 我迷失了。\n",
      "\n",
      "-\n",
      "英文输入: I quit.\n",
      "中文翻译: 我退出。\n",
      "\n",
      "-\n",
      "英文输入: I'm OK.\n",
      "中文翻译: 我沒事。\n",
      "\n",
      "-\n",
      "英文输入: Listen.\n",
      "中文翻译: 听着。\n",
      "\n",
      "-\n",
      "英文输入: No way!\n",
      "中文翻译: 不可能！\n",
      "\n",
      "-\n",
      "英文输入: No way!\n",
      "中文翻译: 不可能！\n",
      "\n",
      "-\n",
      "英文输入: Really?\n",
      "中文翻译: 你确定？\n",
      "\n",
      "-\n",
      "英文输入: Try it.\n",
      "中文翻译: 试试吧。\n",
      "\n",
      "-\n",
      "英文输入: We try.\n",
      "中文翻译: 我们来试试。\n",
      "\n",
      "-\n",
      "英文输入: Why me?\n",
      "中文翻译: 为什么是我？\n",
      "\n",
      "-\n",
      "英文输入: Ask Tom.\n",
      "中文翻译: 去问汤姆。\n",
      "\n",
      "-\n",
      "英文输入: Be calm.\n",
      "中文翻译: 冷静点。\n",
      "\n",
      "-\n",
      "英文输入: Be fair.\n",
      "中文翻译: 公平点。\n",
      "\n",
      "-\n",
      "英文输入: Be kind.\n",
      "中文翻译: 友善点。\n",
      "\n",
      "-\n",
      "英文输入: Be nice.\n",
      "中文翻译: 和气点。\n",
      "\n",
      "-\n",
      "英文输入: Call me.\n",
      "中文翻译: 联系我。\n",
      "\n",
      "-\n",
      "英文输入: Call us.\n",
      "中文翻译: 联系我们。\n",
      "\n",
      "-\n",
      "英文输入: Come in.\n",
      "中文翻译: 进来。\n",
      "\n",
      "-\n",
      "英文输入: Get Tom.\n",
      "中文翻译: 找到汤姆。\n",
      "\n",
      "-\n",
      "英文输入: Get out!\n",
      "中文翻译: 滾出去！\n",
      "\n",
      "-\n",
      "英文输入: Go away!\n",
      "中文翻译: 走開！\n",
      "\n",
      "-\n",
      "英文输入: Go away!\n",
      "中文翻译: 走開！\n",
      "\n",
      "-\n",
      "英文输入: Go away.\n",
      "中文翻译: 走開！\n",
      "\n",
      "-\n",
      "英文输入: Goodbye!\n",
      "中文翻译: 再见！\n",
      "\n",
      "-\n",
      "英文输入: Goodbye!\n",
      "中文翻译: 再见！\n",
      "\n",
      "-\n",
      "英文输入: Hang on!\n",
      "中文翻译: 等一下！\n",
      "\n",
      "-\n",
      "英文输入: He came.\n",
      "中文翻译: 他来了。\n",
      "\n",
      "-\n",
      "英文输入: He runs.\n",
      "中文翻译: 他跑。\n",
      "\n",
      "-\n",
      "英文输入: Help me.\n",
      "中文翻译: 帮我一下。\n",
      "\n",
      "-\n",
      "英文输入: Hold on.\n",
      "中文翻译: 坚持。\n",
      "\n",
      "-\n",
      "英文输入: Hug Tom.\n",
      "中文翻译: 抱抱汤姆！\n",
      "\n",
      "-\n",
      "英文输入: I agree.\n",
      "中文翻译: 我同意。\n",
      "\n",
      "-\n",
      "英文输入: I'm ill.\n",
      "中文翻译: 我生病了。\n",
      "\n",
      "-\n",
      "英文输入: I'm old.\n",
      "中文翻译: 我老了。\n",
      "\n",
      "-\n",
      "英文输入: It's OK.\n",
      "中文翻译: 没关系。\n",
      "\n",
      "-\n",
      "英文输入: It's me.\n",
      "中文翻译: 是我。\n",
      "\n",
      "-\n",
      "英文输入: Join us.\n",
      "中文翻译: 来加入我们吧。\n",
      "\n",
      "-\n",
      "英文输入: Keep it.\n",
      "中文翻译: 留着吧。\n",
      "\n",
      "-\n",
      "英文输入: Kiss me.\n",
      "中文翻译: 吻我。\n",
      "\n",
      "-\n",
      "英文输入: Perfect!\n",
      "中文翻译: 完美！\n",
      "\n",
      "-\n",
      "英文输入: See you.\n",
      "中文翻译: 再见！\n",
      "\n",
      "-\n",
      "英文输入: Shut up!\n",
      "中文翻译: 閉嘴！\n",
      "\n",
      "-\n",
      "英文输入: Skip it.\n",
      "中文翻译: 不管它。\n",
      "\n",
      "-\n",
      "英文输入: Take it.\n",
      "中文翻译: 拿走吧。\n",
      "\n",
      "-\n",
      "英文输入: Wake up!\n",
      "中文翻译: 醒醒！\n",
      "\n",
      "-\n",
      "英文输入: Wash up.\n",
      "中文翻译: 去清洗一下。\n",
      "\n",
      "-\n",
      "英文输入: We know.\n",
      "中文翻译: 我们知道。\n",
      "\n",
      "-\n",
      "英文输入: Welcome.\n",
      "中文翻译: 欢迎。\n",
      "\n",
      "-\n",
      "英文输入: Who won?\n",
      "中文翻译: 谁赢了？\n",
      "\n",
      "-\n",
      "英文输入: Why not?\n",
      "中文翻译: 为什么不？\n",
      "\n",
      "-\n",
      "英文输入: You run.\n",
      "中文翻译: 你跑。\n",
      "\n",
      "-\n",
      "英文输入: Back off.\n",
      "中文翻译: 往后退点。\n",
      "\n",
      "-\n",
      "英文输入: Be still.\n",
      "中文翻译: 静静的，别动。\n",
      "\n",
      "-\n",
      "英文输入: Cuff him.\n",
      "中文翻译: 把他铐上。\n",
      "\n",
      "-\n",
      "英文输入: Drive on.\n",
      "中文翻译: 往前开。\n",
      "\n",
      "-\n",
      "英文输入: Get away!\n",
      "中文翻译: 走開！\n",
      "\n",
      "-\n",
      "英文输入: Get away!\n",
      "中文翻译: 走開！\n",
      "\n",
      "-\n",
      "英文输入: Get down!\n",
      "中文翻译: 趴下！\n",
      "\n",
      "-\n",
      "英文输入: Get lost!\n",
      "中文翻译: 滾！\n",
      "\n",
      "-\n",
      "英文输入: Get real.\n",
      "中文翻译: 醒醒吧。\n",
      "\n",
      "-\n",
      "英文输入: Grab Tom.\n",
      "中文翻译: 抓住汤姆。\n",
      "\n",
      "-\n",
      "英文输入: Grab him.\n",
      "中文翻译: 抓住他。\n",
      "\n",
      "-\n",
      "英文输入: Have fun.\n",
      "中文翻译: 玩得開心。\n",
      "\n",
      "-\n",
      "英文输入: He tries.\n",
      "中文翻译: 他来试试。\n",
      "\n",
      "-\n",
      "英文输入: Humor me.\n",
      "中文翻译: 你就随了我的意吧。\n",
      "\n",
      "-\n",
      "英文输入: Hurry up.\n",
      "中文翻译: 快点！\n",
      "\n",
      "-\n",
      "英文输入: Hurry up.\n",
      "中文翻译: 快点！\n",
      "\n",
      "-\n",
      "英文输入: I forgot.\n",
      "中文翻译: 我忘了。\n",
      "\n",
      "-\n",
      "英文输入: I resign.\n",
      "中文翻译: 我放弃。\n",
      "\n",
      "-\n",
      "英文输入: I'll pay.\n",
      "中文翻译: 我來付錢。\n",
      "\n",
      "-\n",
      "英文输入: I'm busy.\n",
      "中文翻译: 我很忙。\n",
      "\n",
      "-\n",
      "英文输入: I'm cold.\n",
      "中文翻译: 我冷。\n",
      "\n",
      "-\n",
      "英文输入: I'm fine.\n",
      "中文翻译: 我很好。\n",
      "\n",
      "-\n",
      "英文输入: I'm full.\n",
      "中文翻译: 我吃飽了。\n",
      "\n",
      "-\n",
      "英文输入: I'm sick.\n",
      "中文翻译: 我生病了。\n",
      "\n",
      "-\n",
      "英文输入: I'm sick.\n",
      "中文翻译: 我生病了。\n",
      "\n",
      "-\n",
      "英文输入: Leave me.\n",
      "中文翻译: 让我一个人呆会儿。\n",
      "\n",
      "-\n",
      "英文输入: Let's go!\n",
      "中文翻译: 我們走吧!\n",
      "\n",
      "-\n",
      "英文输入: Let's go!\n",
      "中文翻译: 我們走吧!\n",
      "\n",
      "-\n",
      "英文输入: Let's go!\n",
      "中文翻译: 我們走吧!\n",
      "\n",
      "-\n",
      "英文输入: Look out!\n",
      "中文翻译: 当心！\n",
      "\n",
      "-\n",
      "英文输入: She runs.\n",
      "中文翻译: 她跑。\n",
      "\n",
      "-\n",
      "英文输入: Stand up.\n",
      "中文翻译: 起立。\n",
      "\n",
      "-\n",
      "英文输入: They won.\n",
      "中文翻译: 他们赢了。\n",
      "\n",
      "-\n",
      "英文输入: Tom died.\n",
      "中文翻译: 汤姆去世了。\n",
      "\n",
      "-\n",
      "英文输入: Tom quit.\n",
      "中文翻译: 汤姆不干了。\n",
      "\n",
      "-\n",
      "英文输入: Tom swam.\n",
      "中文翻译: 汤姆游泳了。\n",
      "\n",
      "-\n",
      "英文输入: Trust me.\n",
      "中文翻译: 相信我。\n",
      "\n",
      "-\n",
      "英文输入: Try hard.\n",
      "中文翻译: 努力。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # 从训练集中取出前100个样本\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('英文输入:', input_texts[seq_index])\n",
    "    print('中文翻译:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
